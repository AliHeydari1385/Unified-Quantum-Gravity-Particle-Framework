{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdGrcQxzdEXibnimNwE6hF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliHeydari1385/Unified-Quantum-Gravity-Particle-Framework/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "puK7ZMr2DSfO",
        "outputId": "3a937afe-d82d-4d77-8868-757020cba8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.1\n",
            "  Using cached numpy-2.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting emcee\n",
            "  Using cached emcee-3.1.6-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting corner\n",
            "  Using cached corner-2.2.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting camb\n",
            "  Using cached camb-1.6.4-py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting getdist\n",
            "  Using cached getdist-1.7.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting zipfile36\n",
            "  Using cached zipfile36-0.1.3-py3-none-any.whl.metadata (736 bytes)\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas==2.2.2)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==2.2.2)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (109 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting sympy>=1.0 (from camb)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting PyYAML>=5.1 (from getdist)\n",
            "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests)\n",
            "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.2)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.0->camb)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Using cached numpy-2.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "Using cached matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "Using cached emcee-3.1.6-py2.py3-none-any.whl (47 kB)\n",
            "Using cached corner-2.2.3-py3-none-any.whl (15 kB)\n",
            "Using cached camb-1.6.4-py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "Using cached getdist-1.7.2-py3-none-any.whl (833 kB)\n",
            "Using cached zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
            "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "Using cached pyparsing-3.2.4-py3-none-any.whl (113 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: zipfile36, pytz, mpmath, urllib3, tzdata, sympy, six, PyYAML, pyparsing, pillow, packaging, numpy, kiwisolver, idna, fonttools, cycler, charset_normalizer, certifi, scipy, requests, python-dateutil, emcee, contourpy, pandas, matplotlib, camb, getdist, corner\n",
            "  Attempting uninstall: zipfile36\n",
            "    Found existing installation: zipfile36 0.1.3\n",
            "    Uninstalling zipfile36-0.1.3:\n",
            "      Successfully uninstalled zipfile36-0.1.3\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.4\n",
            "    Uninstalling pyparsing-3.2.4:\n",
            "      Successfully uninstalled pyparsing-3.2.4\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.1\n",
            "    Uninstalling numpy-2.0.1:\n",
            "      Successfully uninstalled numpy-2.0.1\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.59.2\n",
            "    Uninstalling fonttools-4.59.2:\n",
            "      Successfully uninstalled fonttools-4.59.2\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.8.3\n",
            "    Uninstalling certifi-2025.8.3:\n",
            "      Successfully uninstalled certifi-2025.8.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: emcee\n",
            "    Found existing installation: emcee 3.1.6\n",
            "    Uninstalling emcee-3.1.6:\n",
            "      Successfully uninstalled emcee-3.1.6\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.6\n",
            "    Uninstalling matplotlib-3.10.6:\n",
            "      Successfully uninstalled matplotlib-3.10.6\n",
            "  Attempting uninstall: camb\n",
            "    Found existing installation: camb 1.6.4\n",
            "    Uninstalling camb-1.6.4:\n",
            "      Successfully uninstalled camb-1.6.4\n",
            "  Attempting uninstall: getdist\n",
            "    Found existing installation: getdist 1.7.2\n",
            "    Uninstalling getdist-1.7.2:\n",
            "      Successfully uninstalled getdist-1.7.2\n",
            "  Attempting uninstall: corner\n",
            "    Found existing installation: corner 2.2.3\n",
            "    Uninstalling corner-2.2.3:\n",
            "      Successfully uninstalled corner-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.2 camb-1.6.4 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 corner-2.2.3 cycler-0.12.1 emcee-3.1.6 fonttools-4.59.2 getdist-1.7.2 idna-3.10 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 numpy-2.0.1 packaging-25.0 pandas-2.2.2 pillow-11.3.0 pyparsing-3.2.4 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.5 scipy-1.16.2 six-1.17.0 sympy-1.14.0 tzdata-2025.2 urllib3-2.5.0 zipfile36-0.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "602106c06103451dab9f0c521e90a04d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful! Proceeding...\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Install packages with force-reinstall to avoid version conflicts\n",
        "!pip install --force-reinstall numpy==2.0.1 scipy pandas==2.2.2 matplotlib emcee corner camb getdist zipfile36 requests\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Manual upload and extract ZIP files\n",
        "print(\"Please upload actlite_3yr_v2p2.zip and Reference.zip\")\n",
        "uploaded = files.upload()  # Upload the ZIP files here (button should appear)\n",
        "\n",
        "# Extract uploaded ZIPs\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        extract_to = f'/content/{filename.split(\".\")[0]}_extract'\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Extracted {filename} to {extract_to}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "4XneTox2DW1o",
        "outputId": "a6785177-b727-42d7-8dd1-a973cda70030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful! Proceeding...\n",
            "Please upload actlite_3yr_v2p2.zip and Reference.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ce953b1-b537-4db8-9003-492341ac4b6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ce953b1-b537-4db8-9003-492341ac4b6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving actlite_3yr_v2p2.zip to actlite_3yr_v2p2.zip\n",
            "Saving Reference.zip to Reference.zip\n",
            "Extracted actlite_3yr_v2p2.zip to /content/actlite_3yr_v2p2_extract\n",
            "Extracted Reference.zip to /content/Reference_extract\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UQGPF v3.1: Enhanced Unified Quantum Gravity Phenomenology Framework\n",
        "# Author: Grok-4 (incorporating user feedback and file handling)\n",
        "# Key Features:\n",
        "# - Reads two ZIP files: actlite_3yr_v2p2.zip and Reference.zip\n",
        "# - Handles nested directory structures within ZIPs using recursive glob.\n",
        "# - Installs necessary libraries (emcee, corner, getdist, pandas, scipy, matplotlib) using subprocess.\n",
        "# - Processes ACT+SPT CMB data, Pantheon+SH0ES SNIa data, and BAO data (placeholder if not found).\n",
        "# - Implements improved physical model (UQGPF v3.0) with curvature, axion, string, neutrino terms.\n",
        "# - Includes sigma clipping for SNIa data.\n",
        "# - Performs MCMC analysis using emcee.\n",
        "# - Generates corner plots, residuals, fit plots, and getdist analysis.\n",
        "# - Creates a final ZIP archive 'UQGPF_Project_v3.1.zip' with all outputs.\n",
        "# - Designed for execution in a new Google Colab notebook.\n",
        "\n",
        "import numpy as np\n",
        "import emcee\n",
        "import corner\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import cholesky, solve_triangular\n",
        "from scipy.integrate import quad, simps\n",
        "from scipy.optimize import minimize\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import sys\n",
        "from getdist import MCSamples, plots\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# --- Installation of necessary libraries ---\n",
        "def install_packages():\n",
        "    packages = ['emcee', 'corner', 'getdist', 'pandas', 'matplotlib', 'scipy']\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            __import__(pkg)\n",
        "            print(f\"{pkg} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {pkg}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "            print(f\"Successfully installed {pkg}.\")\n",
        "\n",
        "# Run installation\n",
        "install_packages()\n",
        "\n",
        "# --- Constants ---\n",
        "c = 299792.458  # km/s\n",
        "H0_ref = 70.0  # km/s/Mpc\n",
        "l_p = 1.616e-35  # Planck length in meters (for scaling)\n",
        "r_d = 147.78  # Sound horizon (Mpc)\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def extract_and_find_files(zip_filename_pattern, extract_subdir, file_patterns):\n",
        "    zip_files = glob.glob(f'{zip_filename_pattern}*.zip')\n",
        "    if not zip_files:\n",
        "        raise FileNotFoundError(f\"Could not find zip file matching pattern: {zip_filename_pattern}\")\n",
        "    zip_path = zip_files[0]\n",
        "    print(f\"Found zip file: {zip_path}\")\n",
        "\n",
        "    if not os.path.exists(extract_subdir):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_subdir)\n",
        "        print(f\"Successfully extracted {zip_path} to {extract_subdir}\")\n",
        "\n",
        "    found_files = []\n",
        "    for pattern in file_patterns:\n",
        "        files = glob.glob(os.path.join(extract_subdir, '**', pattern), recursive=True)\n",
        "        if not files:\n",
        "            raise FileNotFoundError(f\"Could not find file matching pattern: {pattern} in {extract_subdir}\")\n",
        "        found_files.append(files[0])  # Assume first found is correct\n",
        "        print(f\"Found file: {files[0]}\")\n",
        "    return found_files\n",
        "\n",
        "# Extract and load CMB data from actlite_3yr_v2p2.zip\n",
        "cmb_zip_pattern = 'actlite_3yr_v2p2'\n",
        "cmb_extract_dir = 'actlite_3yr_v2p2_extract'\n",
        "cmb_patterns = ['ACT+SPT_cl.dat', 'ACT+SPT_cov.dat']\n",
        "cmb_cl_path, cmb_cov_path = extract_and_find_files(cmb_zip_pattern, cmb_extract_dir, cmb_patterns)\n",
        "\n",
        "# Extract and load SNIa data from Reference.zip\n",
        "ref_zip_pattern = 'Reference'\n",
        "ref_extract_dir = 'reference_extract'\n",
        "snia_patterns = ['ES_AND_COVARPantheon%2BSH0ES.dat.txt']\n",
        "snia_path = extract_and_find_files(ref_zip_pattern, ref_extract_dir, snia_patterns)[0]\n",
        "\n",
        "# Look for BAO data in Reference.zip (adjust pattern if needed; use placeholder if not found)\n",
        "bao_data_path = None\n",
        "bao_files = glob.glob(os.path.join(ref_extract_dir, '**', '*bao*.csv'), recursive=True)  # Example pattern; adjust if real file exists\n",
        "if bao_files:\n",
        "    bao_data_path = bao_files[0]\n",
        "    print(f\"Found BAO data file: {bao_data_path}\")\n",
        "\n",
        "# Loading functions with error handling\n",
        "def load_act_spt_data(cl_path, cov_path):\n",
        "    try:\n",
        "        cl_data = np.loadtxt(cl_path)\n",
        "        ell = cl_data[:, 0]\n",
        "        cl_tt = cl_data[:, 1]\n",
        "        cl_err = cl_data[:, 2] if cl_data.shape[1] > 2 else np.sqrt(np.diag(np.loadtxt(cov_path)))  # Fallback to diag if needed\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error loading CMB cl data from {cl_path}: {e}\")\n",
        "\n",
        "    try:\n",
        "        cov = np.loadtxt(cov_path)\n",
        "        if cov.ndim == 1:\n",
        "            cov = np.diag(cov)\n",
        "        cov = (cov + cov.T) / 2  # Symmetrize\n",
        "        chol_cov = cholesky(cov, lower=True)\n",
        "        inv_cov = solve_triangular(chol_cov, np.eye(len(cov)), lower=True).T @ solve_triangular(chol_cov, np.eye(len(cov)), lower=True)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error loading/processing cov from {cov_path}: {e}. Using diagonal approximation.\")\n",
        "        inv_cov = np.diag(1 / cl_err**2)\n",
        "\n",
        "    return ell, cl_tt, inv_cov, cl_err\n",
        "\n",
        "def load_snia_data(snia_path):\n",
        "    try:\n",
        "        snia_df = pd.read_csv(snia_path, delim_whitespace=True, on_bad_lines='skip')\n",
        "        z = snia_df['zHD'].values if 'zHD' in snia_df.columns else snia_df.iloc[:, 2].values  # Flexible column access\n",
        "        mu = snia_df['MU_SH0ES'].values if 'MU_SH0ES' in snia_df.columns else snia_df.iloc[:, 10].values\n",
        "        mu_err = snia_df['MU_SH0ES_ERR_DIAG'].values if 'MU_SH0ES_ERR_DIAG' in snia_df.columns else snia_df.iloc[:, 11].values\n",
        "        return z, mu, mu_err\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error loading SNIa data from {snia_path}: {e}\")\n",
        "\n",
        "def load_bao_data(bao_path):\n",
        "    if bao_path and os.path.exists(bao_path):\n",
        "        try:\n",
        "            bao_df = pd.read_csv(bao_path)\n",
        "            z_bao = bao_df['z'].values\n",
        "            dm_rd_obs = bao_df['DM_RD'].values\n",
        "            dm_rd_err = bao_df['DM_RD_ERR'].values\n",
        "            return z_bao, dm_rd_obs, dm_rd_err\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Error loading BAO data: {e}. Using placeholder.\")\n",
        "    # Placeholder BAO data\n",
        "    print(\"Using placeholder BAO data.\")\n",
        "    return np.array([0.38, 0.51, 0.61, 2.33]), np.array([10.2, 13.4, 16.1, 37.5]), np.array([0.4, 0.5, 0.6, 1.2])\n",
        "\n",
        "# Load all data\n",
        "ell, cl_obs, inv_cov, cl_err = load_act_spt_data(cmb_cl_path, cmb_cov_path)\n",
        "z_snia_full, mu_obs_full, mu_err_full = load_snia_data(snia_path)\n",
        "z_bao, dm_rd_obs, dm_rd_err = load_bao_data(bao_data_path)\n",
        "\n",
        "# Subsample option for SNIa\n",
        "use_subsample = True  # Set to False for full 1701 points (longer runtime)\n",
        "if use_subsample:\n",
        "    n_snia_subsample = 500\n",
        "    indices = np.random.choice(len(z_snia_full), n_snia_subsample, replace=False)\n",
        "    z_snia = z_snia_full[indices]\n",
        "    mu_obs = mu_obs_full[indices]\n",
        "    mu_err = mu_err_full[indices]\n",
        "    print(f\"Using {n_snia_subsample} subsampled SNIa points.\")\n",
        "else:\n",
        "    z_snia, mu_obs, mu_err = z_snia_full, mu_obs_full, mu_err_full\n",
        "    print(f\"Using full {len(z_snia_full)} SNIa points.\")\n",
        "\n",
        "N_cmb = len(ell)\n",
        "N_snia = len(z_snia)\n",
        "N_bao = len(z_bao)\n",
        "\n",
        "# --- UQGPF Model Functions ---\n",
        "def uqgpf_cmb_cl(ell, theta, scale_factor=1.0):\n",
        "    Omega_m, h, gamma, f_a, m_a, lambda_s, k, ell_damp, nu_cross = theta\n",
        "    term1 = (ell * (ell + 1) / (2 * np.pi)) * (Omega_m ** gamma)\n",
        "    term2 = (m_a / f_a) * np.sin(ell * np.pi / 180)**2 * np.exp(-ell / ell_damp)  # Axion damping\n",
        "    term3 = lambda_s * (l_p**2 / ell**2) * np.cos(ell / 10) * (1 + nu_cross * 1e-38)  # String + neutrino\n",
        "    term4 = k * (ell / 1000)**2 * Omega_m  # Curvature term\n",
        "    return scale_factor * (term1 + term2 + term3 + term4)\n",
        "\n",
        "def hubble_z(z, theta):\n",
        "    Omega_m, h, gamma, f_a, m_a, lambda_s, k, ell_damp, nu_cross = theta\n",
        "    H0 = h * 100\n",
        "    term_matter = Omega_m * (1 + z)**3\n",
        "    term_lambda = (1 - Omega_m)\n",
        "    term_qg = gamma * l_p**2 * (1 + z)**(-2)\n",
        "    term_curvature = k * (c / H0)**2 * (1 + z)**2\n",
        "    term_neutrino = nu_cross * (1 + z)**0.5\n",
        "    return H0 * np.sqrt(term_matter + term_lambda + term_qg + term_curvature + term_neutrino)\n",
        "\n",
        "def uqgpf_mu(z, theta):\n",
        "    H0 = theta[1] * 100\n",
        "    dl = c * (1 + z) / H0 * np.array([simps(1 / (hubble_z(zz, theta) / H0), zz)\n",
        "                                      for zz in [np.linspace(0, zi, 100) for zi in z]])\n",
        "    return 5 * np.log10(dl) + 25\n",
        "\n",
        "# Sigma clipping for SNIa\n",
        "def sigma_clip_snia(z, mu_obs, mu_err, theta, sigma_threshold=3.0):\n",
        "    mu_model = uqgpf_mu(z, theta)\n",
        "    residuals = np.abs(mu_obs - mu_model) / mu_err\n",
        "    mask = residuals < sigma_threshold\n",
        "    return z[mask], mu_obs[mask], mu_err[mask]\n",
        "\n",
        "# Log likelihood with CMB + SNIa + BAO\n",
        "def log_likelihood(theta, ell, cl_obs, inv_cov, z_snia, mu_obs_snia, mu_err_snia, z_bao, dm_rd_obs, dm_rd_err, scale_factor=1.0):\n",
        "    cl_model = uqgpf_cmb_cl(ell, theta, scale_factor)\n",
        "    delta_cl = cl_obs - cl_model\n",
        "    chi2_cmb = np.dot(delta_cl, np.dot(inv_cov, delta_cl))\n",
        "\n",
        "    mu_model = uqgpf_mu(z_snia, theta)\n",
        "    chi2_snia = np.sum(((mu_obs_snia - mu_model) / mu_err_snia)**2)\n",
        "\n",
        "    # BAO chi2\n",
        "    H0 = theta[1] * 100\n",
        "    dm_model = np.array([c * quad(lambda zz: 1 / (hubble_z(zz, theta) / H0), 0, zb)[0] for zb in z_bao]) / r_d\n",
        "    chi2_bao = np.sum(((dm_rd_obs - dm_model) / dm_rd_err)**2)\n",
        "\n",
        "    total_chi2 = chi2_cmb + chi2_snia + chi2_bao\n",
        "    norm_chi2 = total_chi2 / (N_cmb + N_snia + N_bao - len(theta))  # Normalized\n",
        "    return -0.5 * total_chi2, norm_chi2\n",
        "\n",
        "# Log prior (wider ranges)\n",
        "def log_prior(theta):\n",
        "    Omega_m, h, gamma, f_a, m_a, lambda_s, k, ell_damp, nu_cross = theta\n",
        "    if (0.1 < Omega_m < 0.5 and 0.5 < h < 0.9 and 0.1 < gamma < 0.4 and\n",
        "        1e10 < f_a < 1e12 and 1e-10 < m_a < 1e-5 and 0.1 < lambda_s < 10 and\n",
        "        -1.5 < k < 1.5 and 1000 < ell_damp < 5000 and 1e-40 < nu_cross < 1e-36):\n",
        "        return 0.0\n",
        "    return -np.inf\n",
        "\n",
        "# Log probability\n",
        "def log_probability(theta, *args):\n",
        "    lp = log_prior(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    ll, _ = log_likelihood(theta, *args)\n",
        "    return lp + ll\n",
        "\n",
        "# Main MCMC run\n",
        "def run_mcmc(subsample=True, nwalkers=64, nsteps=100000, discard=500, thin=10, scale_factor=1.0):\n",
        "    ndim = 9  # Omega_m, h, gamma, f_a, m_a, lambda_s, k, ell_damp, nu_cross\n",
        "    p0 = np.random.rand(nwalkers, ndim) * 0.1 + np.array([0.3, 0.7, 0.25, 1e11, 1e-8, 1.0, 0.0, 3000, 1e-38])  # Initial guess\n",
        "\n",
        "    # Apply sigma clipping on initial theta\n",
        "    initial_theta = p0[0]\n",
        "    z_snia_clip, mu_obs_clip, mu_err_clip = sigma_clip_snia(z_snia, mu_obs, mu_err, initial_theta)\n",
        "\n",
        "    args = (ell, cl_obs, inv_cov, z_snia_clip, mu_obs_clip, mu_err_clip, z_bao, dm_rd_obs, dm_rd_err, scale_factor)\n",
        "\n",
        "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=args)\n",
        "    try:\n",
        "        sampler.run_mcmc(p0, nsteps, progress=True)\n",
        "    except KeyboardInterrupt:\n",
        "        pass  # Save on interrupt\n",
        "\n",
        "    samples = sampler.get_chain(discard=discard, thin=thin, flat=True)\n",
        "\n",
        "    # Posteriors plot\n",
        "    fig = corner.corner(samples, labels=[\"$\\Omega_m$\", \"$h$\", \"$\\gamma$\", \"$f_a$\", \"$m_a$\", \"$\\lambda_s$\", \"$k$\", \"$\\ell_{damp}$\", \"$\\nu_{cross}$\"])\n",
        "    fig.savefig('uqgpf_posteriors_v3.1.pdf')\n",
        "\n",
        "    # Residuals\n",
        "    best_theta = np.median(samples, axis=0)\n",
        "    cl_model = uqgpf_cmb_cl(ell, best_theta)\n",
        "    residuals_cmb = cl_obs - cl_model\n",
        "    pd.DataFrame({'ell': ell, 'residual': residuals_cmb}).to_csv('residuals_cmb_v3.1.csv', index=False)\n",
        "\n",
        "    mu_model = uqgpf_mu(z_snia_clip, best_theta)\n",
        "    residuals_snia = mu_obs_clip - mu_model\n",
        "    pd.DataFrame({'z': z_snia_clip, 'residual': residuals_snia}).to_csv('residuals_snia_v3.1.csv', index=False)\n",
        "\n",
        "    # Getdist analysis\n",
        "    gdsamples = MCSamples(samples=samples, names=[\"Omega_m\", \"h\", \"gamma\", \"f_a\", \"m_a\", \"lambda_s\", \"k\", \"ell_damp\", \"nu_cross\"])\n",
        "    g = plots.get_subplot_plotter()\n",
        "    g.triangle_plot(gdsamples, filled=True)\n",
        "    g.export('uqgpf_getdist_v3.1.pdf')\n",
        "\n",
        "    # Save chains and ZIP outputs\n",
        "    np.save('mcmc_samples_v3.1.npy', samples)\n",
        "    with zipfile.ZipFile('UQGPF_Project_v3.1.zip', 'w') as zipf:\n",
        "        zipf.write('uqgpf_posteriors_v3.1.pdf')\n",
        "        zipf.write('residuals_cmb_v3.1.csv')\n",
        "        zipf.write('residuals_snia_v3.1.csv')\n",
        "        zipf.write('uqgpf_getdist_v3.1.pdf')\n",
        "        zipf.write('mcmc_samples_v3.1.npy')\n",
        "\n",
        "    return samples, best_theta\n",
        "\n",
        "# Run the code\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    samples, best_theta = run_mcmc(subsample=use_subsample, scale_factor=1.0)  # Adjust scale if needed\n",
        "    print(f\"Best parameters: {best_theta}\")\n",
        "    print(f\"Execution time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "6Etl0K0h74_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}